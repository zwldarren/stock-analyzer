"""DataService domain data service with TTL caching."""

import fnmatch
import logging
import threading
from datetime import date
from typing import TYPE_CHECKING, Any

import pandas as pd
from cachetools import TTLCache

from stock_analyzer.domain.exceptions import DataFetchError
from stock_analyzer.domain.types import (
    ChipDistribution,
    UnifiedRealtimeQuote,
)
from stock_analyzer.infrastructure.data_sources.base import DataFetcherManager
from stock_analyzer.infrastructure.persistence.database import DatabaseManager

if TYPE_CHECKING:
    from stock_analyzer.config import Config

logger = logging.getLogger(__name__)


class DataService:
    """DataService provide unified data access for stock analysis, with caching and multiple data sources."""

    def __init__(
        self,
        stock_repo: DatabaseManager | None = None,
        fetcher_manager: DataFetcherManager | None = None,
        config: "Config | None" = None,
    ):
        """Init DataService with optional repository and fetcher manager"""
        self._stock_repo = stock_repo
        self._fetcher_manager = fetcher_manager
        self._config = config

        self._cache: TTLCache[str, Any] = TTLCache(maxsize=1000, ttl=600)
        self._cache_expiry: dict[str, float] = {}
        self._cache_lock = threading.RLock()

        logger.info("DataService initialized")

    def get_daily_data(
        self,
        stock_code: str,
        days: int = 30,
        target_date: date | None = None,
        use_cache: bool = True,
    ) -> tuple[pd.DataFrame | None, str]:
        """
        Fetch daily stock data with caching strategy

        Strategy:
        1. If use_cache=True, try to get from local DB first
        2. If local data is insufficient or expired, fetch from external API
        3. Save new data to local DB
        """
        if target_date is None:
            target_date = date.today()

        # 1. å°è¯•ä»æœ¬åœ°æ•°æ®åº“è·å–
        if use_cache and self._stock_repo is not None:
            local_data = self._stock_repo.get_daily_data(stock_code, days=days)
            if local_data is not None and not local_data.empty:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æ—¥æœŸæ•°æ®
                latest_date = pd.to_datetime(local_data["date"].iloc[-1]).date()
                if latest_date >= target_date:
                    logger.info(f"[DataService] ä»æœ¬åœ°æ•°æ®åº“è·å– {stock_code} æ•°æ®ï¼Œå…± {len(local_data)} æ¡")
                    return local_data, "database"

        # 2. ä»å¤–éƒ¨æ•°æ®æºè·å–
        if self._fetcher_manager is not None:
            try:
                df, source = self._fetcher_manager.get_daily_data(stock_code, days=days)

                if df is not None and not df.empty:
                    # 3. ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“
                    if self._stock_repo is not None:
                        self._stock_repo.save_daily_data(df, stock_code, data_source=source)
                    logger.info(f"[DataService] ä» {source} è·å– {stock_code} æ•°æ®å¹¶ç¼“å­˜ï¼Œå…± {len(df)} æ¡")
                    return df, source

            except DataFetchError as e:
                logger.error(f"[DataService] è·å– {stock_code} æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
        else:
            logger.warning(f"[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å– {stock_code} çš„å¤–éƒ¨æ•°æ®")

        return None, ""

    def get_realtime_quote(self, stock_code: str) -> UnifiedRealtimeQuote | None:
        """Fetch real-time stock quote with caching strategy"""
        if self._fetcher_manager is None:
            logger.warning(f"[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å– {stock_code} çš„å®æ—¶è¡Œæƒ…")
            return None

        cache_key = f"realtime:{stock_code}"

        # æ£€æŸ¥ç¼“å­˜
        if self._is_cache_valid(cache_key):
            logger.debug(f"[DataService] å®æ—¶è¡Œæƒ…ç¼“å­˜å‘½ä¸­: {stock_code}")
            return self._cache.get(cache_key)

        # ä»æ•°æ®æºè·å–
        quote = self._fetcher_manager.get_realtime_quote(stock_code)

        if quote is not None and self._config is not None:
            # Update cache with configured TTL
            ttl = self._config.realtime_quote.realtime_cache_ttl
            self._set_cache(cache_key, quote, ttl)

        return quote

    def get_chip_distribution(self, stock_code: str) -> ChipDistribution | None:
        """Fetch chip distribution data for a stock, no caching implemented"""
        if self._fetcher_manager is None:
            logger.warning(f"[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å– {stock_code} çš„ç­¹ç åˆ†å¸ƒ")
            return None
        return self._fetcher_manager.get_chip_distribution(stock_code)

    def get_stock_name(self, stock_code: str) -> str | None:
        """Get Chinese stock name by code, with caching strategy"""
        cache_key = f"stock_name:{stock_code}"

        # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        with self._cache_lock:
            if cache_key in self._cache:
                return self._cache[cache_key]

        # 2. å°è¯•ä»å®æ—¶è¡Œæƒ…è·å–
        quote = self.get_realtime_quote(stock_code)
        if quote and hasattr(quote, "name") and quote.name:
            with self._cache_lock:
                self._cache[cache_key] = quote.name
            return quote.name

        # 3. ä»æ•°æ®æºè·å–
        if self._fetcher_manager is None:
            logger.warning(f"[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å– {stock_code} çš„è‚¡ç¥¨åç§°")
            return None

        name = self._fetcher_manager.get_stock_name(stock_code)
        if name:
            with self._cache_lock:
                self._cache[cache_key] = name

        return name

    def batch_get_stock_names(self, stock_codes: list[str]) -> dict[str, str]:
        """Get stock names for a list of stock codes, with caching strategy"""
        result = {}
        missing_codes = []

        # 1. å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        with self._cache_lock:
            for code in stock_codes:
                cache_key = f"stock_name:{code}"
                if cache_key in self._cache:
                    result[code] = self._cache[cache_key]
                else:
                    missing_codes.append(code)

        if not missing_codes:
            return result

        # 2. æ‰¹é‡è·å–å‰©ä½™çš„è‚¡ç¥¨åç§°
        if self._fetcher_manager is None:
            logger.warning("[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•æ‰¹é‡è·å–è‚¡ç¥¨åç§°")
            return result

        names = self._fetcher_manager.batch_get_stock_names(missing_codes)
        result.update(names)

        # 3. æ›´æ–°ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        with self._cache_lock:
            for code, name in names.items():
                self._cache[f"stock_name:{code}"] = name

        return result

    def get_analysis_context(self, stock_code: str, target_date: date | None = None) -> dict[str, Any] | None:
        """Get analysis context for a stock, including today's and yesterday's data, and technical indicators"""
        if target_date is None:
            target_date = date.today()

        # ä»æ•°æ®åº“è·å–æœ€è¿‘2å¤©æ•°æ®
        if self._stock_repo is None:
            logger.warning(f"[DataService] ä»“å‚¨æœªé…ç½®ï¼Œæ— æ³•è·å– {stock_code} çš„æ•°æ®")
            return None

        recent_data = self._stock_repo.get_latest_data(stock_code, days=2)

        if not recent_data:
            logger.warning(f"[DataService] æœªæ‰¾åˆ° {stock_code} çš„æ•°æ®")
            return None

        today_data = recent_data[0]
        yesterday_data = recent_data[1] if len(recent_data) > 1 else None

        context = {
            "code": stock_code,
            "date": today_data.date.isoformat(),
            "today": today_data.to_dict(),
        }

        if yesterday_data:
            context["yesterday"] = yesterday_data.to_dict()

            # è®¡ç®—ç›¸æ¯”æ˜¨æ—¥çš„å˜åŒ–
            if yesterday_data.volume and yesterday_data.volume > 0:
                context["volume_change_ratio"] = round(today_data.volume / yesterday_data.volume, 2)

            if yesterday_data.close and yesterday_data.close > 0:
                context["price_change_ratio"] = round(
                    (today_data.close - yesterday_data.close) / yesterday_data.close * 100, 2
                )

            # å‡çº¿å½¢æ€åˆ¤æ–­
            close = today_data.close
            ma5 = today_data.ma5
            ma10 = today_data.ma10
            ma20 = today_data.ma20
            if close > ma5 > ma10 > ma20 > 0:
                context["ma_status"] = "å¤šå¤´æ’åˆ— ğŸ“ˆ"
            elif close < ma5 < ma10 < ma20 and ma20 > 0:
                context["ma_status"] = "ç©ºå¤´æ’åˆ— ğŸ“‰"
            elif close > ma5 and ma5 > ma10:
                context["ma_status"] = "çŸ­æœŸå‘å¥½ ğŸ”¼"
            elif close < ma5 and ma5 < ma10:
                context["ma_status"] = "çŸ­æœŸèµ°å¼± ğŸ”½"
            else:
                context["ma_status"] = "éœ‡è¡æ•´ç† â†”ï¸"

        return context

    def get_main_indices(self) -> list[dict[str, Any]]:
        """Get main stock indices data"""
        if self._fetcher_manager is None:
            logger.warning("[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å–ä¸»è¦æŒ‡æ•°")
            return []
        return self._fetcher_manager.get_main_indices()

    def get_market_stats(self) -> dict[str, Any]:
        """Get overall market statistics, such asæ¶¨è·Œå®¶æ•°, æˆäº¤é¢, etc."""
        if self._fetcher_manager is None:
            logger.warning("[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å–å¸‚åœºç»Ÿè®¡")
            return {}
        return self._fetcher_manager.get_market_stats()

    def get_sector_rankings(self, n: int = 5) -> tuple[list[dict], list[dict]]:
        """Get sector rankings"""
        if self._fetcher_manager is None:
            logger.warning("[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å–æ¿å—æ’å")
            return [], []
        return self._fetcher_manager.get_sector_rankings(n)

    def get_stock_industry(self, stock_code: str) -> dict[str, Any] | None:
        """
        è·å–è‚¡ç¥¨æ‰€å±è¡Œä¸šä¿¡æ¯

        ä½¿ç”¨efinanceçš„get_base_infoè·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ‰€å¤„è¡Œä¸š

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 

        Returns:
            åŒ…å«è¡Œä¸šä¿¡æ¯çš„å­—å…¸ï¼Œè·å–å¤±è´¥è¿”å›None
        """
        cache_key = f"industry:{stock_code}"

        # æ£€æŸ¥ç¼“å­˜
        if self._is_cache_valid(cache_key):
            logger.debug(f"[DataService] è¡Œä¸šä¿¡æ¯ç¼“å­˜å‘½ä¸­: {stock_code}")
            return self._cache.get(cache_key)

        # ä»æ•°æ®æºè·å–
        if self._fetcher_manager is None:
            logger.warning(f"[DataService] æ•°æ®è·å–å™¨æœªé…ç½®ï¼Œæ— æ³•è·å– {stock_code} çš„è¡Œä¸šä¿¡æ¯")
            return None

        try:
            # å°è¯•ä»efinanceè·å–åŸºæœ¬ä¿¡æ¯
            base_info = self._fetcher_manager.get_base_info(stock_code)
            if base_info:
                industry_info = {
                    "stock_code": stock_code,
                    "industry": base_info.get("æ‰€å¤„è¡Œä¸š"),
                    "sector_code": base_info.get("æ¿å—ç¼–å·"),
                }
                # ç¼“å­˜è¡Œä¸šä¿¡æ¯ï¼ˆ1å°æ—¶TTLï¼‰
                self._set_cache(cache_key, industry_info, 3600)
                logger.info(f"[DataService] è·å– {stock_code} è¡Œä¸šä¿¡æ¯: {industry_info.get('industry')}")
                return industry_info
        except DataFetchError as e:
            logger.warning(f"[DataService] è·å– {stock_code} è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")

        return None

    def get_industry_valuation(self, industry_name: str) -> dict[str, Any] | None:
        """
        è·å–è¡Œä¸šå¹³å‡ä¼°å€¼æ•°æ®ï¼ˆPE/PBï¼‰

        ä½¿ç”¨akshareè·å–ç”³ä¸‡ä¸€çº§è¡Œä¸šçš„å¹³å‡ä¼°å€¼æ•°æ®

        Args:
            industry_name: è¡Œä¸šåç§°ï¼ˆå¦‚"å…‰å­¦å…‰ç”µå­"ã€"é“¶è¡Œ"ç­‰ï¼‰

        Returns:
            åŒ…å«è¡Œä¸šå¹³å‡PE/PBçš„å­—å…¸ï¼Œè·å–å¤±è´¥è¿”å›None
        """
        if not industry_name:
            return None

        cache_key = f"industry_valuation:{industry_name}"

        # æ£€æŸ¥ç¼“å­˜
        if self._is_cache_valid(cache_key):
            logger.debug(f"[DataService] è¡Œä¸šä¼°å€¼ç¼“å­˜å‘½ä¸­: {industry_name}")
            return self._cache.get(cache_key)

        try:
            import akshare as ak

            # è·å–ç”³ä¸‡ä¸€çº§è¡Œä¸šä¿¡æ¯
            df = ak.sw_index_first_info()
            if df is None or df.empty:
                logger.warning("[DataService] æ— æ³•è·å–ç”³ä¸‡è¡Œä¸šæ•°æ®")
                return None

            # æŸ¥æ‰¾åŒ¹é…çš„è¡Œä¸š
            # ç”³ä¸‡è¡Œä¸šæ•°æ®åˆ—ï¼šè¡Œä¸šä»£ç , è¡Œä¸šåç§°, æˆä»½ä¸ªæ•°, é™æ€å¸‚ç›ˆç‡, TTMå¸‚ç›ˆç‡, å¸‚å‡€ç‡, é™æ€è‚¡æ¯ç‡
            industry_row = None
            for _, row in df.iterrows():
                sw_name = str(row.get("è¡Œä¸šåç§°", ""))
                # æ¨¡ç³ŠåŒ¹é…è¡Œä¸šåç§°
                if industry_name in sw_name or sw_name in industry_name:
                    industry_row = row
                    break

            if industry_row is None:
                logger.warning(f"[DataService] æœªæ‰¾åˆ°è¡Œä¸š '{industry_name}' çš„ä¼°å€¼æ•°æ®")
                return None

            # æå–ä¼°å€¼æ•°æ®
            valuation_data = {
                "industry_name": industry_row.get("è¡Œä¸šåç§°"),
                "industry_code": industry_row.get("è¡Œä¸šä»£ç "),
                "component_count": int(industry_row.get("æˆä»½ä¸ªæ•°", 0))
                if pd.notna(industry_row.get("æˆä»½ä¸ªæ•°"))
                else 0,
                "avg_pe_static": float(industry_row.get("é™æ€å¸‚ç›ˆç‡", 0))
                if pd.notna(industry_row.get("é™æ€å¸‚ç›ˆç‡"))
                else 0,
                "avg_pe_ttm": float(industry_row.get("TTM(æ»šåŠ¨)å¸‚ç›ˆç‡", 0))
                if pd.notna(industry_row.get("TTM(æ»šåŠ¨)å¸‚ç›ˆç‡"))
                else 0,
                "avg_pb": float(industry_row.get("å¸‚å‡€ç‡", 0)) if pd.notna(industry_row.get("å¸‚å‡€ç‡")) else 0,
                "dividend_yield": float(industry_row.get("é™æ€è‚¡æ¯ç‡", 0))
                if pd.notna(industry_row.get("é™æ€è‚¡æ¯ç‡"))
                else 0,
            }

            # ç¼“å­˜è¡Œä¸šä¼°å€¼æ•°æ®ï¼ˆ30åˆ†é’ŸTTLï¼Œå› ä¸ºä¼°å€¼æ•°æ®ä¼šéšå¸‚åœºå˜åŒ–ï¼‰
            self._set_cache(cache_key, valuation_data, 1800)
            logger.info(
                f"[DataService] è·å–è¡Œä¸š '{industry_name}' ä¼°å€¼æ•°æ®: "
                f"PE_TTM={valuation_data.get('avg_pe_ttm')}, PB={valuation_data.get('avg_pb')}"
            )
            return valuation_data

        except DataFetchError as e:
            logger.warning(f"[DataService] è·å–è¡Œä¸š '{industry_name}' ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
            return None

    def invalidate_cache(self, pattern: str | None = None) -> None:
        """Invalidate cache entries matching the pattern. If pattern is None, clear all cache."""
        with self._cache_lock:
            if pattern is None:
                self._cache.clear()
                self._cache_expiry.clear()
                logger.info("[DataService] All cache cleared")
            else:
                keys_to_remove = [k for k in self._cache if fnmatch.fnmatch(k, pattern)]
                for key in keys_to_remove:
                    self._remove_cache_entry(key)
                logger.info(f"[DataService] Cache cleared: {pattern} ({len(keys_to_remove)} entries)")

    def _is_cache_valid(self, key: str) -> bool:
        """Check if cache entry is valid (exists and not expired)."""
        with self._cache_lock:
            if key not in self._cache:
                return False
            # Check custom expiry if set
            if key in self._cache_expiry:
                import time

                if time.time() > self._cache_expiry[key]:
                    # Expired, remove from cache
                    self._remove_cache_entry(key)
                    return False
            return True

    def _set_cache(self, key: str, value: Any, ttl_seconds: int) -> None:
        """Set cache entry with TTL (time-to-live) in seconds."""
        import time

        with self._cache_lock:
            self._cache[key] = value
            # Store custom expiry time for this entry
            self._cache_expiry[key] = time.time() + ttl_seconds

    def _remove_cache_entry(self, key: str) -> None:
        """Remove a single cache entry and its expiry record."""
        self._cache.pop(key, None)
        self._cache_expiry.pop(key, None)
